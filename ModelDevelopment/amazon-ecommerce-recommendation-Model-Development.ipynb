{"cells":[{"cell_type":"markdown","metadata":{"id":"x2K3ft7Elr34"},"source":["# E-commerce Product Recommendation System\n","\n","- Name: Nyein Chan Aung\n","- Student ID: st125553"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-q40OQnmO_G"},"outputs":[],"source":["# connect with google\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Mount drive folder with os\n","import os\n","os.chdir('/content/drive/MyDrive/CP4DSAI/_Project/_FinalProject/')"]},{"cell_type":"markdown","metadata":{"id":"svEmn9RUlr37"},"source":["## import required library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IVVU1acQlr38"},"outputs":[],"source":["#Library for edit dataset\n","import pandas as pd\n","import numpy as np\n","import datetime as dp\n","\n","#Library for visualization\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","!pip install plotly\n","\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","\n","!pip install folium\n","import folium\n","from folium.plugins import StripePattern\n","import branca.colormap\n","from collections import defaultdict\n","from folium.plugins import HeatMap\n","\n","#Calculate distance on latitude and longitude\n","from math import radians, cos, sin, asin, sqrt\n","\n","#Library to find correlation in categorical data\n","from pandas import factorize\n","\n","# Modeling\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.cluster import DBSCAN\n","from sklearn.preprocessing import StandardScaler\n","import warnings\n","from sklearn.utils import resample\n","\n","import matplotlib.pyplot as plt\n","# !pip install squarify\n","import squarify\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"Z42y4pIqlr3-"},"source":["### Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqGhPZkXlr3-"},"outputs":[],"source":["cleaned_df = pd.read_csv('data/_cleaned_df.csv')\n","cleaned_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5U4A0wrQlr3_"},"outputs":[],"source":["cleaned_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3w8aZ2SXeDd"},"outputs":[],"source":["len(cleaned_df.cust_id.unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lP4SMH1tXIvo"},"outputs":[],"source":["seg_user_df = pd.read_csv('data/_user_segmentation.csv')\n","seg_user_df = seg_user_df.drop(columns=['Unnamed: 0'])\n","seg_user_df.count()"]},{"cell_type":"markdown","metadata":{"id":"X4FZOcaXlr4C"},"source":["## Feature Selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYK4-Fvjnpsb"},"outputs":[],"source":["# generate pivot table for category with total number of sale\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CEOBtxEKlr4O"},"outputs":[],"source":["# generate the chart to find the relation between discount per cent and total sales in clened_df\n","\n","plt.figure(figsize=(10, 6))\n","plt.scatter(cleaned_df.total, cleaned_df.discount_percent, alpha=0.5)\n","plt.title('Total Amount and Discount Percent')\n","plt.xlabel('Total Amount (usd$)')\n","plt.ylabel('Discount Percent (%)')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WhXtA8Qgof5R"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Set up the figure\n","plt.figure(figsize=(12, 6))\n","\n","# Violin plot to visualize density and distribution of discounts\n","sns.violinplot(data=cleaned_df, x=cleaned_df.category, y=cleaned_df.discount_percent, palette=\"Set3\")\n","plt.title('Violin Plot of Discount Percentages by Category')\n","plt.xlabel('Category')\n","plt.ylabel('Discount Percent (%)')\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","# Show plots\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7RK0fsxAr8Cc"},"outputs":[],"source":["# count the sold category by payment\n","payment_category_count = cleaned_df.groupby('payment_method')['category'].value_counts()\n","\n","# generate multivariate plot for payment category count\n","payment_category_count.unstack().plot(kind='line', stacked=True, figsize=(10, 6))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpJn9u9Ow0UD"},"outputs":[],"source":["# count the sold category by state\n","state_category_count = cleaned_df.groupby('state')['category'].value_counts()\n","\n","# generate multivariate plot for state category count\n","state_category_count.unstack().plot(kind='line', stacked=True, figsize=(12, 6))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuyyFMGwxL-L"},"outputs":[],"source":["# count the sold category by gender\n","gender_category_count = cleaned_df.groupby('gender')['category'].value_counts()\n","\n","# generate plot for gender_category_count\n","gender_category_count.unstack().plot(kind='bar', stacked=True, figsize=(10, 6))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"plK20hgeqkYs"},"outputs":[],"source":["# generate grap for age\n","age_category_count = cleaned_df.groupby('age')['category'].value_counts()\n","\n","# generate plot for gender_category_count\n","age_category_count.unstack().plot(kind='line', stacked=True, figsize=(10, 6))\n"]},{"cell_type":"markdown","metadata":{"id":"MeD2doPqx8Qz"},"source":["According to the analysis, the following features are affected by the sale of category\n","- Age\n","- Payment Method\n","- State\n","- Discount Percent\n","- P.s. There is no significant varient in Gender"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"frKF5jJqg1xD"},"outputs":[],"source":["cleaned_df.category.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Znq1Fctzgxlv"},"outputs":[],"source":["# Merge cluster data and order data\n","merged_df = pd.merge(cleaned_df, seg_user_df, on='cust_id', how='left')\n","merged_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2lCVCluRXcdR"},"outputs":[],"source":["merged_df.k_means_segment.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LC60uHS9haTa"},"outputs":[],"source":["# set filter data with selected features\n","merged_df = merged_df.filter(['cust_id', 'age', 'payment_method', 'state', 'discount_percent', 'k_means_segment', 'category'])\n","merged_df.head()\n"]},{"cell_type":"markdown","metadata":{"id":"69SFB8Rzlr4O"},"source":["## Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtC4aM6yh-DY"},"outputs":[],"source":["merged_df.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9cb2v6siPql"},"outputs":[],"source":["merged_df.cust_id.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"61-VciFxifRc"},"outputs":[],"source":["merged_df.payment_method.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoeyeFBFikNp"},"outputs":[],"source":["merged_df.state.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UlvC4PUCXcdS"},"outputs":[],"source":["len(merged_df.state.unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXsqaaXmipKs"},"outputs":[],"source":["merged_df.discount_percent.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJ0G_tR_ix2Q"},"outputs":[],"source":["merged_df.k_means_segment.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLuJJhAorW1a"},"outputs":[],"source":["merged_df.age.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9umnp57En7Y0"},"outputs":[],"source":["merged_df.info()"]},{"cell_type":"markdown","metadata":{"id":"a52bpgw_nyLi"},"source":["#### Label encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L-ZPkNsNRGl7"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","\n","Payment_encoder = LabelEncoder()\n","State_encoder = LabelEncoder()\n","Category_encoder = LabelEncoder()\n","K_Mean_encoder = LabelEncoder()\n","\n","Percent_scaler = MinMaxScaler()\n","\n","merged_df['payment_method'] = Payment_encoder.fit_transform(merged_df['payment_method'])\n","merged_df['state'] = State_encoder.fit_transform(merged_df['state'])\n","merged_df['k_means_segment'] = K_Mean_encoder.fit_transform(merged_df['k_means_segment'])\n","merged_df['discount_percent'] = Percent_scaler.fit_transform(merged_df[['discount_percent']])\n","merged_df['category'] = Category_encoder.fit_transform(merged_df['category'])\n","\n","merged_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nFl27UtjqPFE"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlHtdiwuqRCV"},"outputs":[],"source":["# split data into X and y\n","X = merged_df.drop(columns=['cust_id', 'category'])\n","X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_JZ7HbYqZ0q"},"outputs":[],"source":["y = merged_df['category']\n","y"]},{"cell_type":"markdown","metadata":{"id":"uxIxqIXTrsoD"},"source":["### Adjust class imbalance by SMOTE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"666X70gBriWr"},"outputs":[],"source":["from imblearn.over_sampling import SMOTE\n","from collections import Counter\n","\n","print(\"Class distribution before oversampling:\", y.value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIWflAixr11O"},"outputs":[],"source":["# Apply SMOTE to oversample the minority class (class 5)\n","smote = SMOTE(sampling_strategy='auto', random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(X, y)\n","\n","# Count the class distribution after oversampling\n","print(\"Class distribution after oversampling:\",  y_resampled.value_counts())"]},{"cell_type":"markdown","metadata":{"id":"M_0YH-DTsIKL"},"source":["#### Split and train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnOXam7asJma"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, train_size=0.8, random_state=42)\n","X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kpOM88NsPa0"},"outputs":[],"source":["y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMNCKjsWsTIM"},"outputs":[],"source":["y_train.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qoFZyOToYvx4"},"outputs":[],"source":["# from sklearn.model_selection import GridSearchCV, train_test_split\n","# from sklearn.ensemble import RandomForestClassifier\n","# from sklearn.linear_model import LogisticRegression\n","# from sklearn.svm import SVC\n","# from sklearn.metrics import classification_report\n","# from sklearn.pipeline import Pipeline\n","# from sklearn.preprocessing import StandardScaler\n","# from xgboost import XGBClassifier\n","# from lightgbm import LGBMClassifier\n","# import pandas as pd\n","\n","\n","# # Split the dataset\n","# X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","# # Define models and hyperparameters\n","# param_grid = {\n","#     # 'LogisticRegression': {\n","#     #     'model': [LogisticRegression(max_iter=500)],\n","#     #     'model__C': [0.1, 1, 10],\n","#     #     'model__penalty': ['l2']\n","#     # },\n","#     # 'RandomForest': {\n","#     #     'model': [RandomForestClassifier(random_state=42)],\n","#     #     'model__n_estimators': [100, 200, 300],\n","#     #     'model__max_depth': [None, 10, 20],\n","#     #     'model__min_samples_split': [2, 5]\n","#     # },\n","#     'SVC': {\n","#         'model': [SVC()],\n","#         'model__C': [0.1, 1, 10],\n","#         'model__kernel': ['linear', 'rbf']\n","#     },\n","#     'XGBClassifier': {\n","#         'model': [XGBClassifier(random_state=42)],\n","#         'model__learning_rate': [0.01, 0.1, 0.2],\n","#         'model__n_estimators': [100, 200],\n","#         'model__max_depth': [3, 5, 7]\n","#     },\n","\n","#     'LightGBM': {\n","#         'pipeline__model': [LGBMClassifier(random_state=42)],\n","#         'pipeline__model__learning_rate': [0.01, 0.1],\n","#         'pipeline__model__n_estimators': [50, 100],\n","#         'pipeline__model__max_depth': [5, 10],\n","#     },\n","# }\n","\n","# # Loop through the parameter grid and find the best model\n","# best_model = None\n","# best_params = None\n","# best_score = 0\n","# results = []\n","\n","# for model_name, grid in param_grid.items():\n","#     print(f\"Running GridSearchCV for {model_name}...\")\n","#     pipeline = Pipeline([\n","#         ('scaler', StandardScaler()),  # Add scaling for algorithms sensitive to feature magnitude\n","#         ('model', grid['model'][0])   # Placeholder for the model\n","#     ])\n","\n","#     # Update grid keys to match pipeline parameters\n","#     grid_search = GridSearchCV(pipeline, param_grid={\n","#         f\"model__{k.split('__')[1]}\": v for k, v in grid.items() if k.startswith('model__')\n","#     }, cv=5, scoring='accuracy', n_jobs=-1)\n","\n","#     # Fit the grid search\n","#     grid_search.fit(X_train, y_train)\n","\n","#     # Record the results\n","#     if grid_search.best_score_ > best_score:\n","#         best_model = model_name\n","#         best_params = grid_search.best_params_\n","#         best_score = grid_search.best_score_\n","\n","#     results.append({\n","#         'Model': model_name,\n","#         'Best Params': grid_search.best_params_,\n","#         'Best CV Score': grid_search.best_score_\n","#     })\n","\n","# # Display results\n","# results_df = pd.DataFrame(results)\n","# print(\"\\nGrid Search Results:\")\n","# print(results_df)\n","\n","# # Best model and parameters\n","# print(f\"\\nBest Model: {best_model}\")\n","# print(f\"Best Parameters: {best_params}\")\n","# print(f\"Best Cross-Validation Score: {best_score}\")\n","\n","# # Evaluate the best model on the test set\n","# final_model = pipeline.set_params(**best_params)\n","# final_model.fit(X_train, y_train)\n","# y_pred = final_model.predict(X_test)\n","\n","# print(\"\\nTest Set Evaluation:\")\n","# print(classification_report(y_test, y_pred))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p60wJBAutZ9W"},"outputs":[],"source":["# from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0Q5sg8Gtbqv"},"outputs":[],"source":["# model = RandomForestClassifier(n_estimators=50, n_jobs=-1, criterion=\"entropy\")\n","# model.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dztXYu_Fbj-p"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_selection import SelectKBest, f_classif\n","import joblib\n","\n","# Feature selection\n","selector = SelectKBest(score_func=f_classif, k=20)  # Reduce to 20 features\n","X_train_selected = selector.fit_transform(X_train, y_train)\n","X_test_selected = selector.transform(X_test)\n","\n","# Train Random Forest with reduced parameters\n","model = RandomForestClassifier(\n","    n_estimators=50,           # Reduce number of trees\n","    max_depth=10,              # Limit depth\n","    min_samples_split=5,       # Prevent overfitting\n","    min_samples_leaf=3,\n","    random_state=42\n",")\n","model.fit(X_train_selected, y_train)\n","\n","# Save model with compression\n","joblib.dump(model, 'model/rf_model_compressed.pkl', compress=('gzip', 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pwOJRS4Ybj-p"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.metrics import classification_report\n","import joblib\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","# Define the pipeline\n","pipeline = Pipeline([\n","    ('scaler', StandardScaler()),                  # Optional: Scale features (may not affect Random Forest)\n","    ('selector', SelectKBest(score_func=f_classif, k=3)),  # Feature selection (adjust k based on your features)\n","    ('rf', RandomForestClassifier(random_state=42, n_jobs=-1))  # Random Forest model\n","])\n","\n","# Adjusted hyperparameter grid for large datasets\n","param_grid = {\n","    'rf__n_estimators': [50, 100, 200],  # Lower number of trees for faster training\n","    'rf__max_depth': [10, 20, None],     # Limit tree depth to control memory usage and overfitting\n","    'rf__min_samples_split': [5, 10],    # Avoid deep splits by increasing minimum samples to split a node\n","    'rf__min_samples_leaf': [2, 4],      # Larger leaf size reduces tree complexity\n","    'rf__max_features': ['sqrt', 'log2']  # Consider fewer features per split for efficiency\n","}\n","\n","# Perform Grid Search CV to tune hyperparameters\n","grid_search = GridSearchCV(\n","    estimator=pipeline,\n","    param_grid=param_grid,\n","    cv=3,                    # Reduce CV folds to save computation time\n","    scoring='accuracy',      # Use accuracy as the scoring metric\n","    n_jobs=-1                # Utilize all available CPU cores\n",")\n","\n","# Fit the pipeline with GridSearchCV\n","grid_search.fit(X_train, y_train)\n","\n","# Output the best model and parameters\n","print(\"Best Parameters:\", grid_search.best_params_)\n","print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n","\n","# Evaluate the final model on the test set\n","y_pred = grid_search.best_estimator_.predict(X_test)\n","print(\"\\nTest Set Evaluation:\")\n","print(classification_report(y_test, y_pred))\n","\n","# Save the final pipeline to disk\n","joblib.dump(grid_search.best_estimator_, 'optimized_random_forest_pipeline.pkl')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MvwIjHItf_L"},"outputs":[],"source":["import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEgryDEwti7z"},"outputs":[],"source":["# with open(\"model/recommender_model.pkl\", \"wb\") as file:\n","#     pickle.dump(model, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3XGskcUtjVP"},"outputs":[],"source":["with open(\"model/payment_encoder.pkl\", \"wb\") as file:\n","    pickle.dump(Payment_encoder, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6NiaHVntpXB"},"outputs":[],"source":["with open(\"model/state_encoder.pkl\", \"wb\") as file:\n","    pickle.dump(State_encoder, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"siOQGOJCt1vD"},"outputs":[],"source":["with open(\"model/category_encoder.pkl\", \"wb\") as file:\n","    pickle.dump(Category_encoder, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMO5Vqjbt1iu"},"outputs":[],"source":["with open(\"model/k_mean_encoder.pkl\", \"wb\") as file:\n","    pickle.dump(K_Mean_encoder, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUDB_gqnt72h"},"outputs":[],"source":["with open(\"model/percent_scaler.pkl\", \"wb\") as file:\n","    pickle.dump(Percent_scaler, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0t-Ab9Tt02R"},"outputs":[],"source":["with open(\"X_test.pkl\", \"wb\") as file:\n","    pickle.dump(X_test, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNlPNU4nubfW"},"outputs":[],"source":["with open(\"y_test.pkl\", \"wb\") as file:\n","    pickle.dump(y_test, file)"]},{"cell_type":"markdown","metadata":{"id":"5FYgZZW_lr4O"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6NCxZBNlr4O"},"outputs":[],"source":["import pickle\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2SLT7BIcumEU"},"outputs":[],"source":["# with open(\"model/recommender_model.pkl\", \"rb\") as file:\n","#     model = pickle.load(file)\n","model = joblib.load('model/rf_model_compressed.pkl')\n","with open(\"model/k_mean_encoder.pkl\", \"rb\") as file:\n","    K_Mean_encoder = pickle.load(file)\n","with open(\"model/payment_encoder.pkl\", \"rb\") as file:\n","    Payment_encoder = pickle.load(file)\n","with open(\"model/state_encoder.pkl\", \"rb\") as file:\n","    State_encoder = pickle.load(file)\n","with open(\"model/category_encoder.pkl\", \"rb\") as file:\n","    Category_encoder = pickle.load(file)\n","with open(\"model/percent_scaler.pkl\", \"rb\") as file:\n","    Percent_encoder = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dYre1Tr5PTa"},"outputs":[],"source":["# age\tpayment_method\tstate\tdiscount_percent\tk_means_segment\n","def get_recommended_products(num = 3):\n","\n","    X = list()\n","    Category_list = dict()\n","    try:\n","        age = input(\"Age: \")\n","        X.append(age)\n","\n","        payment_method = input(\"Payment Method: \")\n","        X.append(Payment_encoder.transform([payment_method])[0])\n","\n","        state = input(\"State: \")\n","        X.append(State_encoder.transform([state])[0])\n","\n","        discount_percent = input(\"Discount Percent: \")\n","        X.append(Percent_encoder.transform([[discount_percent]])[0][0])\n","\n","        customer_type = input(\"Customer Type: \")\n","        X.append(K_Mean_encoder.transform([customer_type])[0])\n","\n","        print(f\"X Value: {X}\")\n","\n","    except ValueError:\n","        print(\"Please enter the correct inputs!\")\n","\n","    for category in Category_encoder.classes_:\n","        # Instead of adding the category as a feature,\n","        # create a separate model or adjust your existing model\n","        # to handle the category separately, perhaps as an input parameter\n","        # or by training the model on data that includes the category as a feature.\n","\n","        # Here's an example of how to exclude the category from the features:\n","        temp_X = X  # Use only the original 5 features\n","\n","        # Reshape temp_X to a 2D array with one row and multiple columns\n","        temp_X = np.array(temp_X).reshape(1, -1)\n","\n","        # Pass temp_X to model.predict\n","        Category_list[category] = model.predict(temp_X)[0]\n","\n","    # print(f\"Recommended products (top {num}): \")\n","    # print(Category_list)\n","\n","    return sorted(Category_list.items(), key=lambda x:x[1], reverse=True)[:num]"]},{"cell_type":"markdown","metadata":{"id":"2fYDyraSXcdc"},"source":["- Age: 34 (int)\n","- PaymentMethod: cod/ Payaxis/ Easypay (string)\n","- State: OK/ FL/ ND (string/ Code)\n","- DiscountPercent: 10 (float)\n","- CustomerType: best/ regular/ loyal (string/Kmean)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hF1mcppL5Rnb"},"outputs":[],"source":["get_recommended_products(num=15)"]},{"cell_type":"markdown","metadata":{"id":"uxE3vUumlr4O"},"source":["## Conclusion"]},{"cell_type":"markdown","metadata":{"id":"XPB-6o2dbj-r"},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}